PML - Programming Assignment
========================================================

This Coursera course teaches us about Practical Machine Learning with R.
We get a good knowledge about various Machine Learning algorithms and useful R packages to made a Data Scientist's day more comfortable. This assessment should show what we have learned so far.

# Abstract
---------------

Quantified Self movement is currently a new trend to improve personal or professional productivity in health and wellness. As discribed in Wikipedia, people have abelities to track physical activity, caloric intake, sleep quality, posture, and other factors involved in personal well-being.
To measure themselves various sensors will be used to collect specific data.

Our goal for this assignment will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

To accomplish this assignment we have to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

### Important points are:

* how we used cross validation
* what is the expected out of sample error
* why we made the choices you did
* predict 20 given different test cases
* a good accuracy

### Minor points are:

* performance
* scalability

I have devided my approach into different steps. I will subscribe each step as good as i can.

## Requirements
Load all necessary libraries used for this project
```{r}
library(caret)
library(randomForest)
library(e1071)
library(doParallel)
```

## Step 1: Loading data
Load the data sets and replace unuseful strings with NAs.
```{r}
trainRawData <- read.csv("data/pml-training.csv", na.strings=c("NA","", "#DIV/0!"))
testingRawData <- read.csv("data/pml-testing.csv", na.strings=c("NA","", "#DIV/0!"))
```

## Step 2: Cleaning up the data

Find columns with NAs set and remove them. This will remove columns which could be imputed too.
I made this as a comromise to speed things up.
```{r}
set.seed(1972)
cleanedData <- trainRawData[ ,colSums(is.na(trainRawData)) == 0]
testing <- testingRawData[ ,colSums(is.na(trainRawData)) == 0]
```

Create a training and cross validation set as shown in the videos.
So we get 70% out of the data for training and 30% for crossvalidation
```{r}
trainIndex <- createDataPartition(y = cleanedData$classe, p=0.7, list=FALSE) # 3927 rows
training <- cleanedData[trainIndex,]
cross <- cleanedData[-trainIndex,]
```

Discard unuseful predictors because they are not numeric.
```{r}
columnsToRemove <- names(training) %in% c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "X", "user_name", "new_window")
training <- training[ , !columnsToRemove]
cross <- cross[ , !columnsToRemove]
testing <- testing[, !columnsToRemove]
```

The cleanup step gaves us 53 predictors to work with.

### Skewness
Lets have a look at the skewness of the data. To find it out we use the skewness function of the 'e1071' package.
```{r}
# check data for skewness
classeName <- names(training) %in% c("classe") 
testForSkewness <- training[!classeName]
# apply the skewnes function to each numeric column of our training set
skewValues <- apply(testForSkewness, 2, skewness)
# create a data frame for fancier printing
skewValuesDf <- data.frame(skewValues)
print(skewValuesDf)
# plot a histogram of the skewness.
hist(skewValues, col=heat.colors(17), xlab="Skewness of all predictors", breaks=20)
```

As you can see we have left-skewness and also right-skewness in our data set. So we need some pre-processing prior to fit our model.
This step will be done directly in the preProcess step of caret's train function.

## Step 3 Model creation

I've trained a Random Forest with 10 K-Folds cross validation partitions in the train control parameter to accomplish this ask.
There was a pre processing step added to normalize the data because it's are skewed as showed above. I've used center and scale to normalize it.

### Build a Random Forest (RF)

#### Advantages of RF:
* very simple to use even with default settings
* produces mostly results with good accuracy without special tuning parameters
* robust
* fast
* can handle larger problems before slowing

#### Disadvantages of RF
* difficult to interpret

Further i've used the doParallel package to speed things up little bit

```{r}
# Create clusters for all available cores communicating over sockets
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)

# global settings used for for all models
#ctrl <- trainControl(method='cv', number=10, savePred=T, classProb=T, verboseIter=T)
ctrl <- trainControl(method='cv', number=10, allowParallel=TRUE)
```

Build a fitted Random Forest model with normalization, and 10-Fold cross vaidation

```{r}
modFitRf <- train(training$classe ~.,
                data = training,
                do.trace=100,
                method="rf",
                trControl=ctrl,
                preProcess=(method=c("center", "scale")))
```

## Results

### Out of Sample accuracy, Random Forest

Now let's take a look at our generated model and it's statistics which looks like as follows:

```{r}
print(modFitRf)
print(modFitRf$finalModel)
```

As you can see the OOB error is ~0.24%. This is a good result.

Predict against our cross validation set created in step 2 to find out the accuracy of our model.
```{r}

predCrossRf <- predict(modFitRf, cross)
print(confusionMatrix(predCrossRf, cross$classe))
```

The confusionMatrix shows us that we have an accuracy of 99.8% with our cross validation set, This is a pretty good result that fits our needs.

The accuracy is good enough to predict the test data set against our Random Forest model.


Print the overall agreement and Kappa:
```{r}
accuracySummary <- postResample(predCrossRf, cross$classe)
print(accuracySummary)
```


## Step 4: Submitted prediction results on the supplied test set 

This is my final result of the prediction that i submitted with my 53 predictors:
```{r}
predRf <- predict(modFitRf, testing)
print(predRf)

# stop all created cluster nodes
stopCluster(cl)

```

## FInal words

I tried to train a Polymomial SVM too, so i can compare two models to each other. Unfortunately it runs a couple of hours and it was not possible for me to build this final document with both models trained via knitr again and again.
It tooks me many, many time to accomplish this task nevertheless i had a lot of fun to play with ML technology.

## References

[1]: http://groupware.les.inf.puc-rio.br/har, Data sets for Human Activity Recognition

[2]: Albert A. Montillo, Ph.D., Guest lecture: Statistical Foundations of Data Analysis
Temple University 4-2-2009, http://www.dabi.temple.edu/~hbling/8590.002/Montillo_RandomForests_4-2-2009.pdf

[3]: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

[4]: Kuhn, M.; Johnson, K. Applied Predictive Modeing, Springer 2013