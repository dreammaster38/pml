<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />

    <title>Coursera - Practica Machine Learning by dreammaster38</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Coursera - Practica Machine Learning</h1>
        <h2>This repository is for the Coursera Practical Machine Learning course</h2>

        <section id="downloads">
          <a href="https://github.com/dreammaster38/pml/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/dreammaster38/pml/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/dreammaster38/pml" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h1>
<a name="pml---programming-assignment" class="anchor" href="#pml---programming-assignment"><span class="octicon octicon-link"></span></a>PML - Programming Assignment</h1>

<p>This Coursera course teaches us about Practical Machine Learning with R.
We get a good knowledge about various Machine Learning algorithms and useful R packages to made a Data Scientist's day more comfortable. This assessment should show what we have learned so far.</p>

<h1>
<a name="abstract" class="anchor" href="#abstract"><span class="octicon octicon-link"></span></a>Abstract</h1>

<hr><p>Quantified Self movement is currently a new trend to improve personal or professional productivity in health and wellness. As discribed in Wikipedia, people have abelities to track physical activity, caloric intake, sleep quality, posture, and other factors involved in personal well-being.
To measure themselves various sensors will be used to collect specific data.</p>

<p>Our goal for this assignment will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>

<p>To accomplish this assignment we have to predict the manner in which they did the exercise. This is the "classe" variable in the training set.</p>

<h3>
<a name="important-points-are" class="anchor" href="#important-points-are"><span class="octicon octicon-link"></span></a>Important points are:</h3>

<ul>
<li>how we used cross validation</li>
<li>what is the expected out of sample error</li>
<li>why we made the choices you did</li>
<li>predict 20 given different test cases</li>
<li>a good accuracy</li>
</ul><h3>
<a name="minor-points-are" class="anchor" href="#minor-points-are"><span class="octicon octicon-link"></span></a>Minor points are:</h3>

<ul>
<li>performance</li>
<li>scalability</li>
</ul><p>I have devided my approach into different steps. I will subscribe each step as good as i can.</p>

<h2>
<a name="requirements" class="anchor" href="#requirements"><span class="octicon octicon-link"></span></a>Requirements</h2>

<p>Load all necessary libraries used for this project</p>

<div class="highlight highlight-r"><pre><span class="kn">library</span><span class="p">(</span>caret<span class="p">)</span>
</pre></div>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<div class="highlight highlight-r"><pre><span class="kn">library</span><span class="p">(</span>randomForest<span class="p">)</span>
</pre></div>

<pre><code>## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<div class="highlight highlight-r"><pre><span class="kn">library</span><span class="p">(</span>e1071<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>doParallel<span class="p">)</span>
</pre></div>

<pre><code>## Loading required package: foreach
## Loading required package: iterators
## Loading required package: parallel
</code></pre>

<h2>
<a name="step-1-loading-data" class="anchor" href="#step-1-loading-data"><span class="octicon octicon-link"></span></a>Step 1: Loading data</h2>

<p>Load the data sets and replace unuseful strings with NAs.</p>

<div class="highlight highlight-r"><pre>trainRawData <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">"data/pml-training.csv"</span><span class="p">,</span> na.strings<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">"NA"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span> <span class="s">"#DIV/0!"</span><span class="p">))</span>
testingRawData <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">"data/pml-testing.csv"</span><span class="p">,</span> na.strings<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">"NA"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span> <span class="s">"#DIV/0!"</span><span class="p">))</span>
</pre></div>

<h2>
<a name="step-2-cleaning-up-the-data" class="anchor" href="#step-2-cleaning-up-the-data"><span class="octicon octicon-link"></span></a>Step 2: Cleaning up the data</h2>

<p>Find columns with NAs set and remove them. This will remove columns which could be imputed too.
I made this as a comromise to speed things up.</p>

<div class="highlight highlight-r"><pre><span class="kp">set.seed</span><span class="p">(</span><span class="m">1972</span><span class="p">)</span>
cleanedData <span class="o">&lt;-</span> trainRawData<span class="p">[</span> <span class="p">,</span><span class="kp">colSums</span><span class="p">(</span><span class="kp">is.na</span><span class="p">(</span>trainRawData<span class="p">))</span> <span class="o">==</span> <span class="m">0</span><span class="p">]</span>
testing <span class="o">&lt;-</span> testingRawData<span class="p">[</span> <span class="p">,</span><span class="kp">colSums</span><span class="p">(</span><span class="kp">is.na</span><span class="p">(</span>trainRawData<span class="p">))</span> <span class="o">==</span> <span class="m">0</span><span class="p">]</span>
</pre></div>

<p>Create a training and cross validation set as shown in the videos.
So we get 70% out of the data for training and 30% for crossvalidation</p>

<div class="highlight highlight-r"><pre>trainIndex <span class="o">&lt;-</span> createDataPartition<span class="p">(</span>y <span class="o">=</span> cleanedData<span class="o">$</span>classe<span class="p">,</span> p<span class="o">=</span><span class="m">0.7</span><span class="p">,</span> <span class="kt">list</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span> <span class="c1"># 3927 rows</span>
training <span class="o">&lt;-</span> cleanedData<span class="p">[</span>trainIndex<span class="p">,]</span>
cross <span class="o">&lt;-</span> cleanedData<span class="p">[</span><span class="o">-</span>trainIndex<span class="p">,]</span>
</pre></div>

<p>Discard unuseful predictors because they are not numeric.</p>

<div class="highlight highlight-r"><pre>columnsToRemove <span class="o">&lt;-</span> <span class="kp">names</span><span class="p">(</span>training<span class="p">)</span> <span class="o">%in%</span> <span class="kt">c</span><span class="p">(</span><span class="s">"raw_timestamp_part_1"</span><span class="p">,</span> <span class="s">"raw_timestamp_part_2"</span><span class="p">,</span> <span class="s">"cvtd_timestamp"</span><span class="p">,</span> <span class="s">"X"</span><span class="p">,</span> <span class="s">"user_name"</span><span class="p">,</span> <span class="s">"new_window"</span><span class="p">)</span>
training <span class="o">&lt;-</span> training<span class="p">[</span> <span class="p">,</span> <span class="o">!</span>columnsToRemove<span class="p">]</span>
cross <span class="o">&lt;-</span> cross<span class="p">[</span> <span class="p">,</span> <span class="o">!</span>columnsToRemove<span class="p">]</span>
testing <span class="o">&lt;-</span> testing<span class="p">[,</span> <span class="o">!</span>columnsToRemove<span class="p">]</span>
</pre></div>

<p>The cleanup step gaves us 53 predictors to work with.</p>

<h3>
<a name="skewness" class="anchor" href="#skewness"><span class="octicon octicon-link"></span></a>Skewness</h3>

<p>Lets have a look at the skewness of the data. To find it out we use the skewness function of the 'e1071' package.</p>

<div class="highlight highlight-r"><pre><span class="c1"># check data for skewness</span>
classeName <span class="o">&lt;-</span> <span class="kp">names</span><span class="p">(</span>training<span class="p">)</span> <span class="o">%in%</span> <span class="kt">c</span><span class="p">(</span><span class="s">"classe"</span><span class="p">)</span> 
testForSkewness <span class="o">&lt;-</span> training<span class="p">[</span><span class="o">!</span>classeName<span class="p">]</span>
<span class="c1"># apply the skewnes function to each numeric column of our training set</span>
skewValues <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span>testForSkewness<span class="p">,</span> <span class="m">2</span><span class="p">,</span> skewness<span class="p">)</span>
<span class="c1"># create a data frame for fancier printing</span>
skewValuesDf <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>skewValues<span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>skewValuesDf<span class="p">)</span>
</pre></div>

<pre><code>##                      skewValues
## num_window            2.310e-02
## roll_belt            -2.050e-03
## pitch_belt           -9.953e-01
## yaw_belt              9.106e-01
## total_accel_belt      4.702e-02
## gyros_belt_x         -5.977e-01
## gyros_belt_y         -6.394e-02
## gyros_belt_z          2.113e-01
## accel_belt_x          9.646e-01
## accel_belt_y          1.767e-01
## accel_belt_z          6.569e-03
## magnet_belt_x         1.429e+00
## magnet_belt_y        -2.229e+00
## magnet_belt_z         2.631e-01
## roll_arm             -1.816e-01
## pitch_arm             1.956e-01
## yaw_arm              -8.960e-02
## total_accel_arm       7.324e-02
## gyros_arm_x          -2.953e-01
## gyros_arm_y           1.179e-01
## gyros_arm_z          -1.644e-01
## accel_arm_x           3.521e-01
## accel_arm_y           8.161e-02
## accel_arm_z          -8.565e-01
## magnet_arm_x         -1.513e-01
## magnet_arm_y         -4.653e-01
## magnet_arm_z         -1.146e+00
## roll_dumbbell        -7.509e-01
## pitch_dumbbell        5.359e-01
## yaw_dumbbell          2.237e-01
## total_accel_dumbbell  5.896e-01
## gyros_dumbbell_x     -1.089e+02
## gyros_dumbbell_y      3.587e+01
## gyros_dumbbell_z      1.148e+02
## accel_dumbbell_x     -4.547e-01
## accel_dumbbell_y      3.438e-01
## accel_dumbbell_z     -7.782e-02
## magnet_dumbbell_x     1.714e+00
## magnet_dumbbell_y    -1.862e+00
## magnet_dumbbell_z     8.682e-01
## roll_forearm         -4.631e-01
## pitch_forearm        -5.278e-01
## yaw_forearm          -2.664e-01
## total_accel_forearm  -5.693e-01
## gyros_forearm_x      -2.683e+00
## gyros_forearm_y       5.417e+01
## gyros_forearm_z       1.024e+02
## accel_forearm_x      -2.375e-01
## accel_forearm_y      -6.435e-01
## accel_forearm_z       4.371e-01
## magnet_forearm_x      6.327e-01
## magnet_forearm_y     -7.478e-01
## magnet_forearm_z     -1.216e+00
</code></pre>

<div class="highlight highlight-r"><pre><span class="c1"># plot a histogram of the skewness.</span>
hist<span class="p">(</span>skewValues<span class="p">,</span> col<span class="o">=</span>heat.colors<span class="p">(</span><span class="m">17</span><span class="p">),</span> xlab<span class="o">=</span><span class="s">"Skewness of all predictors"</span><span class="p">,</span> breaks<span class="o">=</span><span class="m">20</span><span class="p">)</span>
</pre></div>

<p><img src="images/unnamed-chunk-6.png" alt="plot of chunk unnamed-chunk-6"></p>

<p>As you can see we have left-skewness and also right-skewness in our data set. So we need some pre-processing prior to fit our model.
This step will be done directly in the preProcess step of caret's train function.</p>

<h2>
<a name="step-3-model-creation" class="anchor" href="#step-3-model-creation"><span class="octicon octicon-link"></span></a>Step 3 Model creation</h2>

<p>I've trained a Random Forest with 10 K-Folds cross validation partitions in the train control parameter to accomplish this ask.
There was a pre processing step added to normalize the data because it's are skewed as showed above. I've used center and scale to normalize it.</p>

<h3>
<a name="build-a-random-forest-rf" class="anchor" href="#build-a-random-forest-rf"><span class="octicon octicon-link"></span></a>Build a Random Forest (RF)</h3>

<h4>
<a name="advantages-of-rf" class="anchor" href="#advantages-of-rf"><span class="octicon octicon-link"></span></a>Advantages of RF:</h4>

<ul>
<li>very simple to use even with default settings</li>
<li>produces mostly results with good accuracy without special tuning parameters</li>
<li>robust</li>
<li>fast</li>
<li>can handle larger problems before slowing</li>
</ul><h4>
<a name="disadvantages-of-rf" class="anchor" href="#disadvantages-of-rf"><span class="octicon octicon-link"></span></a>Disadvantages of RF</h4>

<ul>
<li>difficult to interpret</li>
</ul><p>Further i've used the doParallel package to speed things up little bit</p>

<div class="highlight highlight-r"><pre><span class="c1"># Create clusters for all available cores communicating over sockets</span>
cl <span class="o">&lt;-</span> makeCluster<span class="p">(</span>detectCores<span class="p">()</span> <span class="o">/</span> <span class="m">2</span><span class="p">)</span>
registerDoParallel<span class="p">(</span>cl<span class="p">)</span>

<span class="c1"># global settings used for for all models</span>
<span class="c1">#ctrl &lt;- trainControl(method='cv', number=10, savePred=T, classProb=T, verboseIter=T)</span>
ctrl <span class="o">&lt;-</span> trainControl<span class="p">(</span>method<span class="o">=</span><span class="s">'cv'</span><span class="p">,</span> number<span class="o">=</span><span class="m">10</span><span class="p">,</span> allowParallel<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</pre></div>

<p>Build a fitted Random Forest model with normalization, and 10-Fold cross vaidation</p>

<div class="highlight highlight-r"><pre>modFitRf <span class="o">&lt;-</span> train<span class="p">(</span>training<span class="o">$</span>classe <span class="o">~</span><span class="m">.</span><span class="p">,</span>
                data <span class="o">=</span> training<span class="p">,</span>
                do.trace<span class="o">=</span><span class="m">100</span><span class="p">,</span>
                method<span class="o">=</span><span class="s">"rf"</span><span class="p">,</span>
                trControl<span class="o">=</span>ctrl<span class="p">,</span>
                preProcess<span class="o">=</span><span class="p">(</span>method<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="s">"center"</span><span class="p">,</span> <span class="s">"scale"</span><span class="p">)))</span>
</pre></div>

<pre><code>## ntree      OOB      1      2      3      4      5
##   100:   0.26%  0.03%  0.53%  0.25%  0.40%  0.24%
##   200:   0.25%  0.05%  0.53%  0.25%  0.36%  0.20%
##   300:   0.25%  0.05%  0.45%  0.25%  0.36%  0.24%
##   400:   0.23%  0.05%  0.41%  0.25%  0.31%  0.24%
##   500:   0.24%  0.05%  0.38%  0.33%  0.31%  0.24%
</code></pre>

<h2>
<a name="results" class="anchor" href="#results"><span class="octicon octicon-link"></span></a>Results</h2>

<h3>
<a name="out-of-sample-accuracy-random-forest" class="anchor" href="#out-of-sample-accuracy-random-forest"><span class="octicon octicon-link"></span></a>Out of Sample accuracy, Random Forest</h3>

<p>Now let's take a look at our generated model and it's statistics which looks like as follows:</p>

<div class="highlight highlight-r"><pre><span class="kp">print</span><span class="p">(</span>modFitRf<span class="p">)</span>
</pre></div>

<pre><code>## Random Forest 
## 
## 13737 samples
##    53 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## Pre-processing: centered, scaled 
## Resampling: Cross-Validated (10 fold) 
## 
## Summary of sample sizes: 12362, 12364, 12363, 12363, 12364, 12363, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##   2     1         1      0.003        0.003   
##   30    1         1      0.002        0.002   
##   50    1         1      0.002        0.003   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.
</code></pre>

<div class="highlight highlight-r"><pre><span class="kp">print</span><span class="p">(</span>modFitRf<span class="o">$</span>finalModel<span class="p">)</span>
</pre></div>

<pre><code>## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry, do.trace = 100) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 27
## 
##         OOB estimate of  error rate: 0.24%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3904    1    0    0    1    0.000512
## B    7 2648    3    0    0    0.003762
## C    0    8 2388    0    0    0.003339
## D    0    0    6 2245    1    0.003108
## E    0    1    0    5 2519    0.002376
</code></pre>

<p>As you can see the OOB error is ~0.24%. This is a good result.</p>

<p>Predict against our cross validation set created in step 2 to find out the accuracy of our model.</p>

<div class="highlight highlight-r"><pre>predCrossRf <span class="o">&lt;-</span> predict<span class="p">(</span>modFitRf<span class="p">,</span> cross<span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>confusionMatrix<span class="p">(</span>predCrossRf<span class="p">,</span> cross<span class="o">$</span>classe<span class="p">))</span>
</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    1    0    0    0
##          B    0 1137    6    0    0
##          C    0    1 1020    5    0
##          D    0    0    0  959    0
##          E    0    0    0    0 1082
## 
## Overall Statistics
##                                         
##                Accuracy : 0.998         
##                  95% CI : (0.996, 0.999)
##     No Information Rate : 0.284         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.997         
##  Mcnemar's Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             1.000    0.998    0.994    0.995    1.000
## Specificity             1.000    0.999    0.999    1.000    1.000
## Pos Pred Value          0.999    0.995    0.994    1.000    1.000
## Neg Pred Value          1.000    1.000    0.999    0.999    1.000
## Prevalence              0.284    0.194    0.174    0.164    0.184
## Detection Rate          0.284    0.193    0.173    0.163    0.184
## Detection Prevalence    0.285    0.194    0.174    0.163    0.184
## Balanced Accuracy       1.000    0.998    0.996    0.997    1.000
</code></pre>

<p>The confusionMatrix shows us that we have an accuracy of 99.8% with our cross validation set, This is a pretty good result that fits our needs.</p>

<p>The accuracy is good enough to predict the test data set against our Random Forest model.</p>

<p>Print the overall agreement and Kappa:</p>

<div class="highlight highlight-r"><pre>accuracySummary <span class="o">&lt;-</span> postResample<span class="p">(</span>predCrossRf<span class="p">,</span> cross<span class="o">$</span>classe<span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>accuracySummary<span class="p">)</span>
</pre></div>

<pre><code>## Accuracy    Kappa 
##   0.9978   0.9972
</code></pre>

<h2>
<a name="step-4-submitted-prediction-results-on-the-supplied-test-set" class="anchor" href="#step-4-submitted-prediction-results-on-the-supplied-test-set"><span class="octicon octicon-link"></span></a>Step 4: Submitted prediction results on the supplied test set</h2>

<p>This is my final result of the prediction that i submitted with my 53 predictors:</p>

<div class="highlight highlight-r"><pre>predRf <span class="o">&lt;-</span> predict<span class="p">(</span>modFitRf<span class="p">,</span> testing<span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>predRf<span class="p">)</span>
</pre></div>

<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

<div class="highlight highlight-r"><pre><span class="c1"># stop all created cluster nodes</span>
stopCluster<span class="p">(</span>cl<span class="p">)</span>
</pre></div>

<h2>
<a name="final-words" class="anchor" href="#final-words"><span class="octicon octicon-link"></span></a>FInal words</h2>

<p>I tried to train a Polymomial SVM too, so i can compare two models to each other. Unfortunately it runs a couple of hours and it was not possible for me to build this final document with both models trained via knitr again and again.
It tooks me many, many time to accomplish this task nevertheless i had a lot of fun to play with ML technology.</p>

<h2>
<a name="references" class="anchor" href="#references"><span class="octicon octicon-link"></span></a>References</h2>

<p>[1]: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>, Data sets for Human Activity Recognition</p>

<p>[2]: Albert A. Montillo, Ph.D., Guest lecture: Statistical Foundations of Data Analysis
Temple University 4-2-2009, <a href="http://www.dabi.temple.edu/%7Ehbling/8590.002/Montillo_RandomForests_4-2-2009.pdf">http://www.dabi.temple.edu/~hbling/8590.002/Montillo_RandomForests_4-2-2009.pdf</a></p>

<p>[3]: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>

<p>[4]: Kuhn, M.; Johnson, K. Applied Predictive Modeing, Springer 2013</p>
      </section>
    </div>

    
  </body>
</html>